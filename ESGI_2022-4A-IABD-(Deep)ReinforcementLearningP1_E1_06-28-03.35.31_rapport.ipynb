{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "rapport.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Rapport Deep Reinforcement learning\n",
    "### Fabien Barrios, Enzo Arhab, Jonathan Duvillage, Bilal Mahjoubi"
   ],
   "metadata": {
    "id": "Vny_r4V8QC71",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## I. Dynamic Programming"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Imports"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from drl_sample_project_python.drl_lib.do_not_touch.mdp_env_wrapper import Env1\n",
    "from drl_sample_project_python.drl_lib.to_do.dynamic_programming import *\n",
    "from drl_sample_project_python.envs.lineworld import LineWorld\n",
    "from drl_sample_project_python.envs.gridworld import GridWorld\n",
    "\n",
    "import time"
   ],
   "metadata": {
    "id": "pfeEyXPRSOsB",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1) Line world\n",
    "\n",
    "Pour le line world, nous devions utilisé le contrat MDP :"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "WmnohRk1QBv6",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "___X___\n"
     ]
    }
   ],
   "source": [
    "lineWorld = LineWorld()\n",
    "lineWorld.view_state(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Le line world est composé de 7 cellules et de deux états terminaux, tout à gauche (position 0) qui correspond à une défaite et tout à droite (position 6) qui correspond à une victoire. On associe à chacun de ces états terminaux un reward (positif ou négatif) selon leur nature. Nous pouvons choisir de nous déplacer soit à gauche, soit à droite, jusqu'à atteindre un état terminal.\n",
    "\n",
    "Nous allons à présent tester une multitude de policy (droite, gauche, aléatoire) sur notre environnement de Line World"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Right-only Policy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.       0.996006 0.997003 0.998001 0.999    1.       0.      ]\n"
     ]
    }
   ],
   "source": [
    "l_right_policy = lineWorld.create_policy(\"right\")\n",
    "print(policy_evaluation_on_line_world(l_right_policy, lineWorld))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Left-only Policy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.       -1.       -0.999    -0.998001 -0.997003 -0.996006  0.      ]\n"
     ]
    }
   ],
   "source": [
    "l_left_policy = lineWorld.create_policy(\"left\")\n",
    "print(policy_evaluation_on_line_world(l_left_policy, lineWorld))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Policy random"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.00000000e+00 -6.66222931e-01 -3.32778545e-01 -2.85554193e-07\n",
      "  3.32778045e-01  6.66222633e-01  0.00000000e+00]\n"
     ]
    }
   ],
   "source": [
    "l_random_policy = lineWorld.create_policy(\"random\")\n",
    "print(policy_evaluation_on_line_world(l_random_policy, lineWorld))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Policy-iteration\n",
    "\n",
    "Nous venons de voir 3 stratégies de jeux basiques en action et avons pour chacune d'entre elle obtenue en sortie la value function.\n",
    "\n",
    "Nous allons maintenant utiliser un algorithme \"Policy-iteration\" afin de trouver la stratégie de jeu optimale et sa value function.\n",
    "A partir d'une stratégie de jeu que nous allons définir (aléatoire, dans notre cas), notre algorithme va suivre cette stratégie de jeu puis évaluer et améliorer notre stratégie de jeu de manière itérative."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]]\n",
      "[0.       0.996006 0.997003 0.998001 0.999    1.       0.      ]\n"
     ]
    }
   ],
   "source": [
    "p_i_rslt = policy_iteration_on_line_world(l_random_policy, lineWorld)\n",
    "print(p_i_rslt.pi)\n",
    "print(p_i_rslt.v)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Après avoir itéré notre algorithme nous renvoie la stratégie de jeu optimale et sa value function. On obtient à nouveau les résultats vu précédemment avec la stratégie de jeu (right-only) qui est dans notre cas de LineWorld la stratégie de jeu optimale."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Value-iteration\n",
    "\n",
    "Contrairement à la Policy-iteration, où nous commençons avec une stratégie de jeu définie, nous commençons ici avec une value function aléatoire, puis de manière équivalente, nous allons évaluer et améliorer notre value function de manière itérative."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]]\n",
      "[0.       0.996006 0.997003 0.998001 0.999    1.       0.      ]\n"
     ]
    }
   ],
   "source": [
    "v_i_rslt = value_iteration_on_line_world(l_random_policy, lineWorld)\n",
    "print(v_i_rslt.pi)\n",
    "print(v_i_rslt.v)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Voila ! Nous retombons à nouveau sur notre policy et value function optimales (right policy).\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2) Grid world\n",
    "\n",
    "Le Grid world correspond à une grille de 5 x 5 cellules. Tout comme le Line world il a également deux états terminaux, le premier tout en haut à droite (position[0][4]) qui correspond à une défaite et le second tout en bas à droite (position[4][4]) qui correspond à la victoire.\n",
    "Nous pouvons nous déplacer dans les 4 directions, en haut, à droite, en bas et à gauche.\n",
    "En ayant notre état terminal de défaite tout à droite, l'on s'assure que la stratégie optimale dans le cas d'un Line world ne pourra pas fonctionner pour notre Grid world."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game Over: False\n",
      "score : 0.0\n",
      "X|_|_|_|_\n",
      "_|_|_|_|_\n",
      "_|_|_|_|_\n",
      "_|_|_|_|_\n",
      "_|_|_|_|_\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gridWorld = GridWorld((5, 5))\n",
    "gridWorld.view()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Nous allons à nouveau, utiliser des algorithmes de Policy-evaluation, Policy-iteration ainsi que de Value-iteration sur notre environnement de Grid world"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Policy-evaluation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game Over: False\n",
      "score : 0.0\n",
      "X|_|_|_|_\n",
      "_|_|_|_|_\n",
      "_|_|_|_|_\n",
      "_|_|_|_|_\n",
      "_|_|_|_|_\n",
      "\n",
      "[[-0.01069167 -0.03257277 -0.09552158 -0.29700697  0.        ]\n",
      " [-0.01023685 -0.0242084  -0.05288917 -0.09269461 -0.02347181]\n",
      " [-0.00608849 -0.00123192  0.00065595  0.00221822  0.00480169]\n",
      " [ 0.01056206  0.02470816  0.05452911  0.09611859  0.02991739]\n",
      " [ 0.01886904  0.0350722   0.09685198  0.2981944   0.        ]]\n"
     ]
    }
   ],
   "source": [
    "g_random_policy = gridWorld.create_policy() # random par défaut\n",
    "print(policy_evaluation_on_grid_world(g_random_policy, gridWorld).reshape((5, 5)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "A l'issue de notre Policy-evaluation nous obtenons la value function associée, bien moins convaincante que celle obtenu durant notre Line world. L'environnement s'étant complexifié, le random a bien moins de réussite. Pour cause, la cellule précédant l'état terminal gagnant avoisine les 0.3"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Policy-iteration"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy-iteration algorithm execution time:  0.4338390827178955 s\n",
      "[[0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "\n",
      "[[0.00713815 0.01176768 0.01346835 0.00872809 0.        ]\n",
      " [0.01681341 0.02651123 0.03343144 0.03494724 0.02939879]\n",
      " [0.03367142 0.04413836 0.05893267 0.06837042 0.04909406]\n",
      " [0.04446979 0.0576147  0.09002625 0.13078133 0.0543337 ]\n",
      " [0.03767691 0.05205498 0.1131367  0.31091853 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "p_i_start = time.time()\n",
    "p_i_rslt = policy_iteration_on_grid_world(g_random_policy, gridWorld)\n",
    "print(\"Policy-iteration algorithm execution time: \",time.time() - p_i_start, \"s\")\n",
    "print(p_i_rslt.pi, end=\"\\n\\n\")\n",
    "print(p_i_rslt.v.reshape((5, 5)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Nous obtenons en sorti une stratégie de jeu optimale, notre stratégie de jeu nous renvoie la direction à prendre pour chacune des cases de la grille.\n",
    "Exemple avec la première case :\n",
    "```\n",
    "p_i_rslt.pi[0] = [0. 1. 0. 0.] # Première case\n",
    "pi[0][0] signifie aller à gauche\n",
    "pi[0][1] signifie aller à droite\n",
    "pi[0][2] signifie aller en haut\n",
    "pi[0][3] signifie aller en bas\n",
    "```\n",
    "Dans un environnement où l'agent part de la première case, la stratégie optimale est d'aller à droite 3 fois, puis de descendre de 4 cellules jusqu'à arriver à l'état terminal gagnant.\n",
    "Dans notre cas, les directions en haut et à gauche ne sont pas utilisées."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Value-iteration"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value-iteration algorithm execution time:  0.05986309051513672 s\n",
      "[[0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "\n",
      "[[0.99302097 0.99401498 0.99500999 0.996006   0.        ]\n",
      " [0.99401498 0.99500999 0.996006   0.997003   0.996006  ]\n",
      " [0.99500999 0.996006   0.997003   0.998001   0.997003  ]\n",
      " [0.996006   0.997003   0.998001   0.999      0.998001  ]\n",
      " [0.997003   0.998001   0.999      1.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "v_i_start = time.time()\n",
    "v_i_rslt = value_iteration_on_grid_world(g_random_policy, gridWorld) # theta = 1e-7\n",
    "print(\"Value-iteration algorithm execution time: \",time.time() - v_i_start, \"s\")\n",
    "print(v_i_rslt.pi, end=\"\\n\\n\")\n",
    "print(v_i_rslt.v.reshape((5, 5)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Tout comme le Line World, nous appliquons sur le Grid World la value iteration. Nous pouvons voir que nous retombons sur la stratégie de jeu optimale.\n",
    "\n",
    "En terme de comparaison de performance, dans notre cas assez simplet du Grid world, l'algorithme Policy-iteration s'est exécuté en 0.0928s tandis que Value-iteration en 0.0935s. On considère alors que nos deux algorithmes sont similaires dans notre cas du Grid world.\n",
    "\n",
    "Il nous est également difficile de les comparer, les temps d'exécutions pouvant beaucoup fluctuer selon les exécutions."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3) Environnement secret"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4]\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "secret_env1 = Env1()\n",
    "S = secret_env1.states()\n",
    "\n",
    "print(S) # Ensemble des states posssibles\n",
    "print(len(secret_env1.actions())) # Nombre d'actions possibles"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "states_actions = (len(S), len(secret_env1.actions()))\n",
    "s_random_policy = np.random.random(states_actions)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Policy-evaluation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.51273112 0.51356384 0.49990268 0.62419102 0.        ]\n"
     ]
    }
   ],
   "source": [
    "print(policy_evaluation_on_secret_env1(s_random_policy))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Notre environnement secret ne comporte qu'un seul état terminal (position[4])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Policy-iteration"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy-iteration algorithm execution time:  0.002992391586303711 s\n",
      "[[1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]]\n",
      "\n",
      "[0.66666669 1.         1.         1.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "p_i_start = time.time()\n",
    "p_i_rslt = policy_iteration_on_secret_env1(s_random_policy)\n",
    "print(\"Policy-iteration algorithm execution time: \",time.time() - p_i_start, \"s\")\n",
    "print(p_i_rslt.pi, end=\"\\n\\n\")\n",
    "print(p_i_rslt.v)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Value-iteration"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value-iteration algorithm execution time:  0.0019943714141845703 s\n",
      "[[1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]]\n",
      "\n",
      "[0.66666669 1.         1.         1.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "v_i_start = time.time()\n",
    "v_i_rslt = value_iteration_on_secret_env1(s_random_policy)\n",
    "print(\"Value-iteration algorithm execution time: \",time.time() - v_i_start, \"s\")\n",
    "print(v_i_rslt.pi, end=\"\\n\\n\")\n",
    "print(v_i_rslt.v)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## II] Méthodes de Monte Carlo"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Les méthodes de Monte Carlo ont été plus difficiles à implémenter contrairement aux algorithmes de Dynamic Programming."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Imports"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "from drl_sample_project_python.drl_lib.do_not_touch.single_agent_env_wrapper import Env2\n",
    "from drl_sample_project_python.envs.tictactoe import TicTacToe, all_states_ttt\n",
    "from drl_lib.to_do.monte_carlo_methods import *"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3) Tic Tac Toe"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Afin de tester nos différentes implémentations de Monte Carlo, il nous a été demandé d'implémenter un environnement Tic Tac Toe."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|_||_||_|\n",
      "|_||_||_|\n",
      "|_||_||_|\n",
      "[0 0 0 0 0 0 0 0 0]\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "ttt = TicTacToe()\n",
    "ttt.view()\n",
    "print(ttt.board)\n",
    "print(type(ttt.board))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Monte Carlo Exploring Starts"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|X||X||O|\n",
      "|O||X||X|\n",
      "|X||X||O|\n",
      "[[0.11111111 0.11111111 0.11111111 ... 0.11111111 0.11111111 0.11111111]\n",
      " [0.         0.         0.         ... 0.         1.         0.        ]\n",
      " [0.         1.         0.         ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.11111111 0.11111111 0.11111111 ... 0.11111111 0.11111111 0.11111111]\n",
      " [0.11111111 0.11111111 0.11111111 ... 0.11111111 0.11111111 0.11111111]\n",
      " [0.11111111 0.11111111 0.11111111 ... 0.11111111 0.11111111 0.11111111]]\n",
      "[[0.40195471 0.62575408 0.29528111 ... 0.59475911 0.03355135 0.87215005]\n",
      " [0.50265995 0.88202314 0.59128148 ... 0.37866633 0.92434962 0.74745501]\n",
      " [0.59580714 0.61788651 0.60306249 ... 0.54334016 0.44704328 0.11179533]\n",
      " ...\n",
      " [0.58773865 0.36964766 0.90123784 ... 0.01564148 0.89374847 0.30584848]\n",
      " [0.01288941 0.75524041 0.82808278 ... 0.64410479 0.01935815 0.89530773]\n",
      " [0.119657   0.43621744 0.06405846 ... 0.31606597 0.21840722 0.18132104]]\n"
     ]
    }
   ],
   "source": [
    "p_avf_MC_ES = monte_carlo_es_on_tic_tac_toe_solo(10000)\n",
    "print(p_avf_MC_ES.pi)\n",
    "print(p_avf_MC_ES.q)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "A remplir"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### On Policy First Visit Monte Carlo Control"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|O||O||O|\n",
      "|O||O||X|\n",
      "|O||X||O|\n",
      "[[0.11111111 0.11111111 0.11111111 ... 0.11111111 0.11111111 0.11111111]\n",
      " [0.00125    0.99125    0.00125    ... 0.00125    0.00125    0.00125   ]\n",
      " [0.11111111 0.11111111 0.11111111 ... 0.11111111 0.11111111 0.11111111]\n",
      " ...\n",
      " [0.11111111 0.11111111 0.11111111 ... 0.11111111 0.11111111 0.11111111]\n",
      " [0.11111111 0.11111111 0.11111111 ... 0.11111111 0.11111111 0.11111111]\n",
      " [0.11111111 0.11111111 0.11111111 ... 0.11111111 0.11111111 0.11111111]]\n",
      "[[ 0.69133228  0.17767583  0.07260931 ...  0.27235916  0.60505658\n",
      "   0.66987668]\n",
      " [ 0.10334415  0.91803989  0.16120516 ... -0.4985015   0.38611893\n",
      "   0.86757685]\n",
      " [ 0.5826545   0.34259876  0.39027276 ...  0.24081973  0.07910792\n",
      "   0.56565449]\n",
      " ...\n",
      " [ 0.95750208  0.41272464  0.08734643 ...  0.39725423  0.82287595\n",
      "   0.50171372]\n",
      " [ 0.24573647  0.47576868  0.59263855 ...  0.51808399  0.13871653\n",
      "   0.00387677]\n",
      " [ 0.25547911  0.26994893  0.86589398 ...  0.78191044  0.32172244\n",
      "   0.89658134]]\n"
     ]
    }
   ],
   "source": [
    "p_avf_onP_FV_MC_control = on_policy_first_visit_monte_carlo_control_on_tic_tac_toe_solo(10000)\n",
    "print(p_avf_onP_FV_MC_control.pi)\n",
    "print(p_avf_onP_FV_MC_control.q)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "A remplir"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Off Policy Monte Carlo Control"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]]\n",
      "[[0.93894298 0.4975192  0.70500015 ... 0.88987596 0.77708924 0.2066583 ]\n",
      " [0.70053014 0.67482763 0.54425136 ... 0.43469455 0.80377925 0.06637946]\n",
      " [0.57068623 0.45098649 0.05578165 ... 0.64009066 0.62075356 0.26640363]\n",
      " ...\n",
      " [0.50668762 0.13296827 0.76778746 ... 0.30312136 0.06793464 0.32290086]\n",
      " [0.9660801  0.27984577 0.60669162 ... 0.09337648 0.66281144 0.20212981]\n",
      " [0.76626618 0.40655086 0.99197686 ... 0.18451125 0.18026057 0.18297079]]\n"
     ]
    }
   ],
   "source": [
    "p_avf_offP_MC_control = off_policy_monte_carlo_control_on_tic_tac_toe_solo(10000)\n",
    "print(p_avf_offP_MC_control.pi)\n",
    "print(p_avf_offP_MC_control.q)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ]
}